{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook conducts an Exploratory Data Analysis (EDA) on the standardized data that is produced from the methods of the Data-Cleaning notebook.\n\n## Import packages:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import gresearch_crypto\n\nfrom datetime import datetime\nimport gc\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as md\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:15:46.128885Z","iopub.execute_input":"2022-03-22T22:15:46.129281Z","iopub.status.idle":"2022-03-22T22:15:47.227053Z","shell.execute_reply.started":"2022-03-22T22:15:46.129178Z","shell.execute_reply":"2022-03-22T22:15:47.226451Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Declare variables","metadata":{}},{"cell_type":"code","source":"train_filepath = '/kaggle/input/g-research-crypto-forecasting/train.csv'\nasset_details_filepath = '/kaggle/input/g-research-crypto-forecasting/asset_details.csv'","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:15:47.228447Z","iopub.execute_input":"2022-03-22T22:15:47.228785Z","iopub.status.idle":"2022-03-22T22:15:47.232562Z","shell.execute_reply.started":"2022-03-22T22:15:47.228756Z","shell.execute_reply":"2022-03-22T22:15:47.231510Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Declare Cleaning Functions","metadata":{}},{"cell_type":"code","source":"def clean_dates(df):\n    '''\n    Function to clean timestamps of an individual coin \n    by the earliest and latest timestamps observed for that coin.\n    Also fills in missing values by the method chosen for the interpolate function.\n    \n    Inputs: \n        df (pd.DataFrame.GroupBy object) :\n            Grouped Dataframe by unique coins. All timestamps must be\n            in intervals of 60 seconds.\n\n    Outputs:\n        constant_dates_df (pd.DataFrame) :\n            Dataframe with timestamps and filled missing values.\n    '''\n    \n    df = df.copy()\n    \n    dates = range(min(df[\"timestamp\"]), max(df[\"timestamp\"]), 60)\n    \n    df.set_index(\"timestamp\", inplace = True)\n    \n    df = df.reindex(dates)\n    \n    # replace the 9 infinite values with NaN, which are interpolated in the next line\n    df.replace([np.inf, -np.inf], np.nan, inplace = True)\n    \n    # don't fill missing values at end of dataset, where they do not\n    # have ending observations to interpolate with\n    df.interpolate(method = \"linear\", inplace = True, limit_area = \"inside\")\n    \n    return df\n    \ndef standardize_data(df):\n    '''\n    Function to standardize data by creating rows for every timestamp\n    and subsetting to only consider when all coins had their first observation made.\n    \n    Inputs:\n        df (pd.DataFrame) :\n            Time series data to be standardized\n            \n    Outputs:\n        standard_df (pd.DataFrame) :\n            Time series data now standardized\n    '''\n    \n    # deep copy to not alter the original\n    df = df.copy()\n    \n    # fill missing rows / values between coin's start and stop date\n    # Note: Missing rows beyond an individual coin's start / stop date are not created,\n    # only those between are filled in\n    standard_df = df.groupby(\"Asset_ID\").apply(clean_dates).reset_index(level = 0, drop = True)\n    \n    # reset twice so timestamp is only a column and not also index, \n    # this makes each entry have a unique index\n    standard_df = standard_df.reset_index()\n    \n    # get the earliest timestamp for each coin, then get the latest timestamp out of those.\n    # this shows when the latest coin was introduced, after which there are observations for\n    # all coins\n    first_timestamp = max(standard_df.groupby(\"Asset_ID\")[\"timestamp\"].min())\n    \n    # subset to only consider the time period where observations existed for all coins\n    standard_df = standard_df.loc[standard_df[\"timestamp\"] >= first_timestamp]\n    \n    # drop ending rows with missing values\n    standard_df.dropna(inplace = True)\n    \n    return standard_df","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:15:47.233908Z","iopub.execute_input":"2022-03-22T22:15:47.234174Z","iopub.status.idle":"2022-03-22T22:15:47.245462Z","shell.execute_reply.started":"2022-03-22T22:15:47.234143Z","shell.execute_reply":"2022-03-22T22:15:47.244837Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Import and Clean Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(train_filepath)\nasset_details_df = pd.read_csv(asset_details_filepath)\n\nstandard_df = standardize_data(df)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:15:47.246515Z","iopub.execute_input":"2022-03-22T22:15:47.246885Z","iopub.status.idle":"2022-03-22T22:17:44.716542Z","shell.execute_reply.started":"2022-03-22T22:15:47.246855Z","shell.execute_reply":"2022-03-22T22:17:44.715644Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"standard_df","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:17:44.718831Z","iopub.execute_input":"2022-03-22T22:17:44.719077Z","iopub.status.idle":"2022-03-22T22:17:44.747402Z","shell.execute_reply.started":"2022-03-22T22:17:44.719046Z","shell.execute_reply":"2022-03-22T22:17:44.746442Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"asset_details_df","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:17:44.749080Z","iopub.execute_input":"2022-03-22T22:17:44.749407Z","iopub.status.idle":"2022-03-22T22:17:44.760412Z","shell.execute_reply.started":"2022-03-22T22:17:44.749364Z","shell.execute_reply":"2022-03-22T22:17:44.759731Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Stationarity EDA","metadata":{}},{"cell_type":"code","source":"standard_df.loc[:, \"datetime\"] = pd.to_datetime(standard_df.loc[:, \"timestamp\"].values, \n                                                unit = \"s\", infer_datetime_format = True)\n\nfig, axes = plt.subplots(nrows = int(len(asset_details_df)/2), ncols = 2, \n                         sharex = True, figsize = (10,10))\naxes = axes.flatten()\n\nfor ax, asset in zip(axes, asset_details_df[\"Asset_ID\"]):\n    standard_df.loc[standard_df[\"Asset_ID\"] == asset].plot(x = \"datetime\",\n        y = \"Target\", ax = ax, legend = False, xlabel = \"\", ylabel = \"\",\n        title = asset_details_df.loc[asset_details_df[\"Asset_ID\"] == asset, \"Asset_Name\"].iloc[0])\n    ax.axhline(y = standard_df.loc[standard_df[\"Asset_ID\"] == asset, \"Target\"].mean(), \n             xmin = 0, xmax = 1, color = \"r\")\n    \nfig.suptitle(\"Figure 1. Target Over Time\")\nfig.text(0.5, 0.04, 'Time', ha='center')\nfig.text(0.04, 0.5, 'Target', va='center', rotation='vertical')\nfig.subplots_adjust(hspace=.5)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:17:44.761455Z","iopub.execute_input":"2022-03-22T22:17:44.761681Z","iopub.status.idle":"2022-03-22T22:19:44.537164Z","shell.execute_reply.started":"2022-03-22T22:17:44.761654Z","shell.execute_reply":"2022-03-22T22:19:44.536171Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"del fig\ndel axes\ndel df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:19:44.538375Z","iopub.execute_input":"2022-03-22T22:19:44.538694Z","iopub.status.idle":"2022-03-22T22:19:47.290834Z","shell.execute_reply.started":"2022-03-22T22:19:44.538662Z","shell.execute_reply":"2022-03-22T22:19:47.290066Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Shown above, the values of target during the course of the standardized dataset appear stationary by visual appearance. \n\nThey can be formally investigated with a Augmented Dickey-Fuller hypothesis test.","metadata":{}},{"cell_type":"code","source":"# manual loop to increase time but decrease memory\nfor asset in asset_details_df[\"Asset_ID\"]:\n    \n    gc.collect()\n    \n    print(asset_details_df.loc[asset_details_df[\"Asset_ID\"] == asset, \"Asset_Name\"].iloc[0])\n    \n    # get every nth row for test with full lags\n    n = 20\n    maxlags = 20\n    \n    # conduct tests and get p-value\n    \n    print(\"\\tFull Lags on Subset of Data:\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Target\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"\\tFull Lags on Random Sample of Data:\")\n    # full lags but random sample of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Target\"].sample(frac = 1/n).sort_index(), \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"\\tFirst Lags on Full Data:\")\n    # first lags but full data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Target\"], \n                   maxlag = maxlags, regression = \"nc\")[1])\n    print(\"\")\n    \n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:19:47.292172Z","iopub.execute_input":"2022-03-22T22:19:47.292471Z","iopub.status.idle":"2022-03-22T22:45:46.153333Z","shell.execute_reply.started":"2022-03-22T22:19:47.292418Z","shell.execute_reply":"2022-03-22T22:45:46.152413Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Above, it can clearly be seen that all data subsetting methods for all coins result in a p-value of near zero. Therefore, the null hypothesis that an individual series is not stationary is rejected. The alternative hypothesis that an individual series is stationary is failed to be rejected.\n\nTo better determine if these data subsetting methods produce similar results, and to take the most conservative for use in the future, the test statistics can be viewed for the most popular coins, Bitcoin and Ethereum.","metadata":{}},{"cell_type":"code","source":"# manual loop to increase time but decrease memory\nfor asset in [1, 6]:\n    \n    gc.collect()\n    \n    print(asset_details_df.loc[asset_details_df[\"Asset_ID\"] == asset, \"Asset_Name\"].iloc[0])\n    # get every nth row for test with full lags\n    n = 20\n    maxlags = 20\n    \n    # conduct tests and get p-value\n    \n    print(\"\\tFull Lags on Subset of Data:\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Target\"].iloc[::n], \n                   regression = \"nc\")[0])\n    \n    gc.collect()\n    \n    print(\"\\tFull Lags on Random Sample of Data:\")\n    # full lags but random sample of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Target\"].sample(frac = 1/n).sort_index(), \n                   regression = \"nc\")[0])\n    \n    gc.collect()\n    \n    print(\"\\tFirst Lags on Full Data:\")\n    # first lags but full data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Target\"], \n                   maxlag = maxlags, regression = \"nc\")[0])\n    print(\"\")\n    \n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:45:46.154915Z","iopub.execute_input":"2022-03-22T22:45:46.155207Z","iopub.status.idle":"2022-03-22T22:49:25.645151Z","shell.execute_reply.started":"2022-03-22T22:45:46.155163Z","shell.execute_reply":"2022-03-22T22:49:25.644152Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that the smallest test statistics, which is least likely to reject the null hypothesis and thus be the most conservative, is when using the test with full lags but on a subset of the data based on repeated intervals. Therefore, this method should be used when evaluating future tests on other aspects of the data.\n\nIt will also be important to know if other variables are stationary, which will now be evaluated with this method.","metadata":{}},{"cell_type":"code","source":"print(\"\\tFull Lags on Subset of Data:\")\n# manual loop to increase time but decrease memory\nfor asset in asset_details_df[\"Asset_ID\"]:\n    \n    gc.collect()\n    print()\n    print(asset_details_df.loc[asset_details_df[\"Asset_ID\"] == asset, \"Asset_Name\"].iloc[0])\n    \n    # get every nth row for test with full lags\n    n = 20\n    \n    # conduct tests and get p-value\n    \n    print(\"Open\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Open\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"High\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"High\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"Low\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Low\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"Close\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Close\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"Volume\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"Volume\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"VWAP\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"VWAP\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T22:49:25.646436Z","iopub.execute_input":"2022-03-22T22:49:25.646669Z","iopub.status.idle":"2022-03-22T23:25:14.993133Z","shell.execute_reply.started":"2022-03-22T22:49:25.646639Z","shell.execute_reply":"2022-03-22T23:25:14.992195Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Every coin is non-stationary in all other continuous variables except for Volume. Further, for a single coin each non-stationary variable has about the same p-value. \n\nTo turn these series stationary, different transformations should be developed coin by coin. Because these variables exhibit the same degree of non-stationarity, only one variable will be examined for each to develop transformations, assuming that these will also make the other variables for the coin stationary as well. Opening price will be used for this purpose.\n\nFirst, Open will be visualized for each coin.","metadata":{}},{"cell_type":"code","source":"gc.collect()\n\nfig, axes = plt.subplots(nrows = int(len(asset_details_df)/2), ncols = 2, \n                         sharex = True, figsize = (10,10))\naxes = axes.flatten()\n\nfor ax, asset in zip(axes, asset_details_df[\"Asset_ID\"]):\n    standard_df.loc[standard_df[\"Asset_ID\"] == asset].plot(x = \"datetime\",\n        y = \"Open\", ax = ax, legend = False, xlabel = \"\", ylabel = \"\",\n        title = asset_details_df.loc[asset_details_df[\"Asset_ID\"] == asset, \"Asset_Name\"].iloc[0])\n    ax.axhline(y = standard_df.loc[standard_df[\"Asset_ID\"] == asset, \"Open\"].mean(), \n             xmin = 0, xmax = 1, color = \"r\")\n    \nfig.suptitle(\"Figure 2. Open Price Over Time\")\nfig.text(0.5, 0.04, 'Time', ha='center')\nfig.text(0.04, 0.5, 'Open Price (USD)', va='center', rotation='vertical')\nfig.subplots_adjust(hspace=.5)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T23:25:14.994422Z","iopub.execute_input":"2022-03-22T23:25:14.994746Z","iopub.status.idle":"2022-03-22T23:27:19.171015Z","shell.execute_reply.started":"2022-03-22T23:25:14.994703Z","shell.execute_reply":"2022-03-22T23:27:19.170135Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Clearly, increasing prices of crypto currencies over time creates a positive trend. With added speculation, variance increases over time as well. \n\nThis can each be addressed with first differencing and a logarithm transformation respectively.","metadata":{}},{"cell_type":"code","source":"gc.collect()\n\nstandard_df[\"open_t\"] = (np.log(standard_df[\"Open\"].copy())).diff()\nstandard_df.fillna(0, inplace = True)\n\nfig, axes = plt.subplots(nrows = int(len(asset_details_df)/2), ncols = 2, \n                         sharex = True, figsize = (10,10))\naxes = axes.flatten()\n\nfor ax, asset in zip(axes, asset_details_df[\"Asset_ID\"]):\n    standard_df.loc[standard_df[\"Asset_ID\"] == asset].plot(x = \"datetime\",\n        y = \"open_t\", ax = ax, legend = False, xlabel = \"\", ylabel = \"\",\n        title = asset_details_df.loc[asset_details_df[\"Asset_ID\"] == asset, \"Asset_Name\"].iloc[0])\n    ax.axhline(y = standard_df.loc[standard_df[\"Asset_ID\"] == asset, \"open_t\"].mean(), \n             xmin = 0, xmax = 1, color = \"r\")\n    \nfig.suptitle(\"Figure 3. Transformed Open Price Over Time\")\nfig.text(0.5, 0.04, 'Time', ha='center')\nfig.text(0.04, 0.5, 'Transformed Open Price', va='center', rotation='vertical')\nfig.subplots_adjust(hspace=.5)\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T23:27:19.172324Z","iopub.execute_input":"2022-03-22T23:27:19.172567Z","iopub.status.idle":"2022-03-22T23:31:38.546794Z","shell.execute_reply.started":"2022-03-22T23:27:19.172537Z","shell.execute_reply":"2022-03-22T23:31:38.545844Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Shown above, a few outliers caused from the first difference operation drastically changes the scale of the y-axis.\n\nThe same stationarity tests can now be repeated for the transformed series for a numerical evaluation.","metadata":{}},{"cell_type":"code","source":"gc.collect()\n\nstandard_df[\"close_t\"] = (np.log(standard_df[\"Close\"])).diff()\nstandard_df[\"high_t\"] = (np.log(standard_df[\"High\"])).diff()\nstandard_df[\"low_t\"] = (np.log(standard_df[\"Low\"])).diff()\nstandard_df[\"vwap_t\"] = (np.log(standard_df[\"VWAP\"])).diff()\nstandard_df.fillna(0, inplace = True)\n\nprint(\"\\tFull Lags on Subset of Data:\")\n# manual loop to increase time but decrease memory\nfor asset in asset_details_df[\"Asset_ID\"]:\n    \n    gc.collect()\n    print()\n    print(asset_details_df.loc[asset_details_df[\"Asset_ID\"] == asset, \"Asset_Name\"].iloc[0])\n    \n    # get every nth row for test with full lags\n    n = 20\n    \n    # conduct tests and get p-value\n    \n    print(\"Open\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"open_t\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"High\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"high_t\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"Low\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"low_t\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"Close\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"close_t\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()\n    \n    print(\"VWAP\")\n    # full lags but subset of data\n    print(adfuller(standard_df.loc[(standard_df[\"Asset_ID\"] == asset), \"vwap_t\"].iloc[::n], \n                   regression = \"nc\")[1])\n    \n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T23:31:38.550126Z","iopub.execute_input":"2022-03-22T23:31:38.550469Z","iopub.status.idle":"2022-03-23T00:01:40.761148Z","shell.execute_reply.started":"2022-03-22T23:31:38.550420Z","shell.execute_reply":"2022-03-23T00:01:40.760346Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Now, all of these variables for all coins are stationary suggesting that the logarithm and first difference transformation is adequate. ","metadata":{}}]}